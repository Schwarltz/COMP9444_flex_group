{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path = './imagenette2/train/'\n",
    "img_height = 150\n",
    "img_width = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([\n",
    "  transforms.Resize((img_height,img_width)),\n",
    "  transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root=training_dataset_path, transform=training_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_calc(loader):\n",
    "  mean = 0.\n",
    "  std = 0.\n",
    "  total_img_count = 0\n",
    "  for images, _ in loader:\n",
    "    image_count_in_batch = images.size(0)\n",
    "    images = images.view(image_count_in_batch, images.size(1),-1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_img_count += image_count_in_batch\n",
    "\n",
    "    if total_img_count % 1024 == 0:\n",
    "      print(f'Read {total_img_count} images.')\n",
    "\n",
    "  mean /= total_img_count \n",
    "  std /= total_img_count\n",
    "  print(f'Read in total {total_img_count} images.')\n",
    "  return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1024 images.\n",
      "Read 2048 images.\n",
      "Read 3072 images.\n",
      "Read 4096 images.\n",
      "Read 5120 images.\n",
      "Read 6144 images.\n",
      "Read 7168 images.\n",
      "Read 8192 images.\n",
      "Read 9216 images.\n",
      "Read in total 9469 images.\n",
      "(tensor([0.4626, 0.4580, 0.4295]), tensor([0.2314, 0.2250, 0.2337]))\n"
     ]
    }
   ],
   "source": [
    "print(mean_std_calc(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d53ec6266a26bc8bdfb2a6f99d7ada5f09e50f34a6a9bd31567b89d722776c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
