{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix2 Original using ReLU6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "    # basically resnet block\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False), # convolution on itself; in_channels->middle_channels\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False), # downsamples if stride=2 (e.g. 56^2 -> 28^2)\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False), # convolution on itself again; middle_channels-> out_channels\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU6(),\n",
    "    )\n",
    "\n",
    "    # MobileNetV2 block but normal convolution first\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=3, stride=1, padding=1, bias=False), # std convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False), # depth-wise convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False), # point wise convolution\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU6(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    # structure based off MobileNetV2 table\n",
    "    # [bs, 3, 224, 224]\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.ReLU6()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    # [bs, 32, 112,112]\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    # [bs, 16, 112,112]\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    # [bs, 24, 56, 56]\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    # [bs, 32, 28, 28]\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    # [bs, 64, 14, 14]\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    # [bs, 96, 14, 14]\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    # [bs, 160, 7, 7]\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    # [bs, 320, 7, 7]\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    # [bs, 1280, 7, 7]\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    # 1280 nodes\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix2 Mish Model Partial Dropout\n",
    "\n",
    "Changed ReLU6 to Mish.\n",
    "Appears to converge much faster in the early stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "   \n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False), # convolution on itself; in_channels->middle_channels\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False), # downsamples if stride=2 (e.g. 56^2 -> 28^2)\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False), # convolution on itself again; middle_channels-> out_channels\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    \n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=3, stride=1, padding=1, bias=False), # std convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False), # depth-wise convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False), # point wise convolution\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    # structure based off MobileNetV2 table\n",
    "    # [bs, 3, 224, 224]\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    # [bs, 32, 112,112]\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1) \n",
    "    # [bs, 16, 112,112]\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    # [bs, 24, 56, 56]\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    # [bs, 32, 28, 28]\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    # [bs, 64, 14, 14]\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    # [bs, 96, 14, 14]\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    # [bs, 160, 7, 7]\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    # [bs, 320, 7, 7]\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    # [bs, 1280, 7, 7]\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    # 1280 nodes\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix2 Mish with Dropout\n",
    "\n",
    "Dropout added within blocks with probabilty of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "    d_prob = 0.1\n",
    "\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False), # self conv\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False), # downsamples if stride=2\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, in_channels*expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(in_channels*expansion),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(in_channels*expansion, in_channels*expansion, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False),\n",
    "        nn.BatchNorm2d(in_channels*expansion),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(in_channels*expansion, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix2 with proper mobilenetv2 block implemented\n",
    "\n",
    "Changed the first convolution in pass2 to a self convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=1, stride=1, padding=0, bias=False), # changed this from a std 3x3 convolution to a 1x1 0 padding convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Residual Mix2 Network\n",
    "\n",
    "Modified the _make_layer function and made the residual pass forward more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "    x += identity # always adds identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1 or in_channels != out_channels: # identity convolution now occurs at least once in every call\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = Network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d53ec6266a26bc8bdfb2a6f99d7ada5f09e50f34a6a9bd31567b89d722776c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
