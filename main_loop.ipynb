{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt       \n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# Google Colab\n",
    "#from fastai.vision.all import *\n",
    "#set_seed(42, reproducible= True)\n",
    "#source = untar_data(URLs.IMAGENETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"Tench\", \"English Springer\", \"Cassette Player\", \"Chain Saw\", \"Church\", \"French Horn\", \"Garbage Truck\", \"Gas Pump\", \"Golf Ball\", \"Parachute\")\n",
    "\n",
    "def load_data():\n",
    "  img_dir = 'imagenette2/'\n",
    "\n",
    "  train = os.path.join(img_dir, 'train')\n",
    "  val = os.path.join(img_dir, 'val')\n",
    "\n",
    "\n",
    "  train_dataset = ImageFolder(\n",
    "    train,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224), \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4655, 0.4546, 0.4251), (0.2775, 0.2725, 0.2938)),\n",
    "        transforms.RandomErasing()\n",
    "    ]))\n",
    "\n",
    "  test_dataset = ImageFolder(\n",
    "    val,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4655, 0.4546, 0.4251), (0.2775, 0.2725, 0.2938))\n",
    "    ]))\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=20)\n",
    "  \n",
    "  return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super().__init__()\n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        shortcut = self.shortcut(input)\n",
    "        input = nn.ReLU()(self.bn1(self.conv1(input)))\n",
    "        input = self.dropout(input)\n",
    "        input = nn.ReLU()(self.bn2(self.conv2(input)))\n",
    "        input = self.dropout(input)\n",
    "        input = input + shortcut\n",
    "        input = nn.ReLU()(input)\n",
    "        input = self.dropout(input)\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, in_channels, resblock, outputs=10):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            resblock(64, 64, downsample=False),\n",
    "            resblock(64, 64, downsample=False),\n",
    "            resblock(64, 64, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            resblock(64, 128, downsample=True),\n",
    "            resblock(128, 128, downsample=False),\n",
    "            resblock(128, 128, downsample=False),\n",
    "            resblock(128, 128, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            resblock(128, 256, downsample=True),\n",
    "            resblock(256, 256, downsample=False),\n",
    "            resblock(256, 256, downsample=False),\n",
    "            resblock(256, 256, downsample=False),\n",
    "            resblock(256, 256, downsample=False),\n",
    "            resblock(256, 256, downsample=False)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            resblock(256, 512, downsample=True),\n",
    "            resblock(512, 512, downsample=False),\n",
    "            resblock(512, 512, downsample=False),\n",
    "        )\n",
    "\n",
    "        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = torch.nn.Linear(512, outputs)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.layer0(input)\n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input)\n",
    "        input = self.layer3(input)\n",
    "        input = self.layer4(input)\n",
    "        input = self.gap(input)\n",
    "        input = torch.flatten(input,start_dim=1)\n",
    "        input = self.dropout(input)\n",
    "        input = self.fc(input)\n",
    "\n",
    "        return F.log_softmax(input, dim=1)\n",
    "\n",
    "# Use saved model\n",
    "path = \"./84resnet34.pth\"\n",
    "model = ResNet34(3, ResBlock)\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ResBottleneckBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, downsample):\n",
    "#         super().__init__()\n",
    "#         self.downsample = downsample\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels//4, kernel_size=1, stride=1)\n",
    "#         self.conv2 = nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, stride=2 if downsample else 1, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(out_channels//4, out_channels, kernel_size=1, stride=1)\n",
    "#         self.shortcut = nn.Sequential()\n",
    "        \n",
    "#         if self.downsample or in_channels != out_channels:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2 if self.downsample else 1),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels//4)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels//4)\n",
    "#         self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "#         self.dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         shortcut = self.shortcut(input)\n",
    "#         input = nn.ReLU()(self.bn1(self.conv1(input)))\n",
    "#         input = self.dropout(input)\n",
    "#         input = nn.ReLU()(self.bn2(self.conv2(input)))\n",
    "#         input = self.dropout(input)\n",
    "#         input = nn.ReLU()(self.bn3(self.conv3(input)))\n",
    "#         input = self.dropout(input)\n",
    "#         input = input + shortcut\n",
    "#         input = nn.ReLU()(input)\n",
    "#         input = self.dropout(input)\n",
    "#         return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, in_channels, resblock, repeat, useBottleneck=False, outputs=10):\n",
    "#         super().__init__()\n",
    "#         self.layer0 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#         if useBottleneck:\n",
    "#             filters = [64, 256, 512, 1024, 2048]\n",
    "#         else:\n",
    "#             filters = [64, 64, 128, 256, 512]\n",
    "\n",
    "#         self.layer1 = nn.Sequential()\n",
    "#         self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n",
    "#         for i in range(1, repeat[0]):\n",
    "#                 self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n",
    "\n",
    "#         self.layer2 = nn.Sequential()\n",
    "#         self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n",
    "#         for i in range(1, repeat[1]):\n",
    "#                 self.layer2.add_module('conv3_%d' % (i+1,), resblock(filters[2], filters[2], downsample=False))\n",
    "\n",
    "#         self.layer3 = nn.Sequential()\n",
    "#         self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n",
    "#         for i in range(1, repeat[2]):\n",
    "#             self.layer3.add_module('conv2_%d' % (i+1,), resblock(filters[3], filters[3], downsample=False))\n",
    "\n",
    "#         self.layer4 = nn.Sequential()\n",
    "#         self.layer4.add_module('conv5_1', resblock(filters[3], filters[4], downsample=True))\n",
    "#         for i in range(1, repeat[3]):\n",
    "#             self.layer4.add_module('conv3_%d'%(i+1,), resblock(filters[4], filters[4], downsample=False))\n",
    "\n",
    "#         self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc = torch.nn.Linear(filters[4], outputs)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         input = self.layer0(input)\n",
    "#         input = self.layer1(input)\n",
    "#         input = self.layer2(input)\n",
    "#         input = self.layer3(input)\n",
    "#         input = self.layer4(input)\n",
    "#         input = self.gap(input)\n",
    "#         input = torch.flatten(input, start_dim=1)\n",
    "#         input = self.dropout(input)\n",
    "#         input = self.fc(input)\n",
    "#         output = F.log_softmax(input, dim=1)\n",
    "#         return output\n",
    "        \n",
    "# path = \"./resnet101(77).pth\"\n",
    "# #model = ResNet(3, ResBottleneckBlock, [3, 4, 6, 3], useBottleneck=True, outputs=10)\n",
    "\n",
    "# model = ResNet(3, ResBottleneckBlock, [3, 4, 23, 3], useBottleneck=True, outputs=10)\n",
    "# model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Function to save the model\n",
    "def saveModel():\n",
    "    path = \"./classifier_full.pth\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def testAccuracy(device):\n",
    "    \n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)\n",
    "\n",
    "def trainAccuracy(device):\n",
    "\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)\n",
    "\n",
    "def train(num_epochs):\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Define your execution device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "    # Convert model parameters and buffers to CPU or Cuda\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define the loss function with Classification Cross-Entropy loss and an optimizer with Adam optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.00001, weight_decay = 0.00001)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    all_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader, 0):\n",
    "            # get the inputs\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # predict classes using images from the training set\n",
    "            outputs = model(images)\n",
    "            # compute the loss based on model output and real labels\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            # adjust parameters based on the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Let's print statistics for every 50 images\n",
    "            running_loss += loss.item()     # extract the loss value\n",
    "            if i % 1000 == 999:    \n",
    "                # print every 50 (twice per epoch) \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 1000))\n",
    "                # zero the loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Compute and print the average accuracy for this epoch when tested over all test images\n",
    "        accuracy = testAccuracy(device)\n",
    "        train_accuracy = trainAccuracy(device)\n",
    "        all_accuracy.append(accuracy)\n",
    "        print('For epoch', epoch+1,'the train accuracy is %d %%' % (train_accuracy), 'the test accuracy over the whole test set is %d %%' % (accuracy))\n",
    "        \n",
    "        # we want to save the model if the accuracy is the best\n",
    "        if accuracy > best_accuracy:\n",
    "            saveModel()\n",
    "            best_accuracy = accuracy\n",
    "    print('The average accuracy over %d' % num_epochs, ' runs is %.2f' % (sum(all_accuracy)/num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show the images\n",
    "def imageshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    if torch.cuda.is_available():\n",
    "      npimg = img.cpu().numpy()\n",
    "    else:\n",
    "      npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkTransformedImages():\n",
    "    # get batch of images from the test DataLoader\n",
    "    images, labels = next(iter(test_loader))\n",
    "    if torch.cuda.is_available():\n",
    "      images = images.cuda()\n",
    "      labels = labels.cuda()\n",
    "\n",
    "    # show all images as one image grid\n",
    "    #imageshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cuda:0 device\n",
      "For epoch 1 the train accuracy is 79 % the test accuracy over the whole test set is 77 %\n",
      "For epoch 2 the train accuracy is 87 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 3 the train accuracy is 88 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 4 the train accuracy is 88 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 5 the train accuracy is 89 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 6 the train accuracy is 89 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 7 the train accuracy is 89 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 8 the train accuracy is 89 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 9 the train accuracy is 88 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 10 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 11 the train accuracy is 90 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 12 the train accuracy is 89 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 13 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 14 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 15 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 16 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 17 the train accuracy is 90 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 18 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 19 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 20 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 21 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 22 the train accuracy is 90 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 23 the train accuracy is 92 % the test accuracy over the whole test set is 84 %\n",
      "For epoch 24 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 25 the train accuracy is 91 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 26 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 27 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 28 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 29 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 30 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 31 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 32 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 33 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 34 the train accuracy is 92 % the test accuracy over the whole test set is 82 %\n",
      "For epoch 35 the train accuracy is 92 % the test accuracy over the whole test set is 84 %\n",
      "For epoch 36 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 37 the train accuracy is 91 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 38 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 39 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 40 the train accuracy is 93 % the test accuracy over the whole test set is 84 %\n",
      "For epoch 41 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 42 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 43 the train accuracy is 92 % the test accuracy over the whole test set is 84 %\n",
      "For epoch 44 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n",
      "For epoch 45 the train accuracy is 92 % the test accuracy over the whole test set is 83 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\caspar\\OneDrive - UNSW\\Desktop\\UNSW\\COMP9444\\imagenette2\\main_loop.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=0'>1</a>\u001b[0m train_loader, test_loader \u001b[39m=\u001b[39m load_data()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=2'>3</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=3'>4</a>\u001b[0m train(num_epochs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=5'>6</a>\u001b[0m checkTransformedImages()\n",
      "\u001b[1;32mc:\\Users\\caspar\\OneDrive - UNSW\\Desktop\\UNSW\\COMP9444\\imagenette2\\main_loop.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=97'>98</a>\u001b[0m         running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=99'>100</a>\u001b[0m \u001b[39m# Compute and print the average accuracy for this epoch when tested over all test images\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=100'>101</a>\u001b[0m accuracy \u001b[39m=\u001b[39m testAccuracy(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=101'>102</a>\u001b[0m train_accuracy \u001b[39m=\u001b[39m trainAccuracy(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=102'>103</a>\u001b[0m all_accuracy\u001b[39m.\u001b[39mappend(accuracy)\n",
      "\u001b[1;32mc:\\Users\\caspar\\OneDrive - UNSW\\Desktop\\UNSW\\COMP9444\\imagenette2\\main_loop.ipynb Cell 15\u001b[0m in \u001b[0;36mtestAccuracy\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=13'>14</a>\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=15'>16</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=17'>18</a>\u001b[0m         images, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/caspar/OneDrive%20-%20UNSW/Desktop/UNSW/COMP9444/imagenette2/main_loop.ipynb#ch0000014?line=18'>19</a>\u001b[0m         images \u001b[39m=\u001b[39m Variable(images\u001b[39m.\u001b[39mto(device))\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 230\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\folder.py:249\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    248\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(f)\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:934\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mPalette\u001b[39m.\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    936\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    938\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\ImageFile.py:239\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         s \u001b[39m=\u001b[39m read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecodermaxblock)\n\u001b[0;32m    240\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, struct\u001b[39m.\u001b[39merror) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    241\u001b[0m         \u001b[39m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         \u001b[39mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[1;32mc:\\Users\\caspar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\JpegImagePlugin.py:402\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_read\u001b[39m(\u001b[39mself\u001b[39m, read_bytes):\n\u001b[0;32m    397\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[39m    internal: read more image data\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(read_bytes)\n\u001b[0;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m ImageFile\u001b[39m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_ended\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Premature EOF.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[0;32m    407\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data()\n",
    "\n",
    "num_epochs = 50\n",
    "train(num_epochs)\n",
    "\n",
    "checkTransformedImages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "788e8838e60568927ffa64b6b77efdbb7f35845aa27b966fb4ed5665ee0c16f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
