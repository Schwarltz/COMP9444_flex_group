{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ykfIYL3YnuSe",
        "outputId": "cd4de423-b6bd-4677-bbc5-682215eff5f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1. Select a device \\n2. Prepare data\\n3. Build neural network\\n4. Training\\n5. Evaluate  \\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1. Select a device \n",
        "2. Prepare data\n",
        "3. Build neural network\n",
        "4. Training\n",
        "5. Evaluate  \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "svO4X0r04KSn",
        "outputId": "baae8af6-318b-42e5-e318-894717f51dee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='1557168128' class='' max='1557161267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [1557168128/1557161267 01:57<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Select a device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from fastai.vision.all import *\n",
        "set_seed(42,reproducible=True)\n",
        "source = untar_data(URLs.IMAGENETTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Iwf9NfP-4TS7"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "transformer = transforms.Compose([\n",
        "    transforms.Resize((150, 150)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQeFF5-54lSs",
        "outputId": "09e2e966-f11b-42e2-cc2e-a7e2ad1026dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['n01440764', 'n02102040', 'n02979186', 'n03000684', 'n03028079', 'n03394916', 'n03417042', 'n03425413', 'n03445777', 'n03888257']\n"
          ]
        }
      ],
      "source": [
        "# Prepare data\n",
        "train_path = source/'train'\n",
        "val_path = source/'val'\n",
        "train_loader = DataLoader(\n",
        "    torchvision.datasets.ImageFolder(root=train_path, transform=transformer),\n",
        "    batch_size = 128,\n",
        "    shuffle = True  \n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    torchvision.datasets.ImageFolder(root=val_path, transform=transformer),\n",
        "    batch_size = 128\n",
        ")\n",
        "\n",
        "\"\"\"This gets the names of the classes (names of folders containing classes)\"\"\" \n",
        "root = pathlib.Path(train_path)\n",
        "classes = sorted([i.name.split('/')[-1] for i in root.iterdir()])\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "daJMl6du4mG6"
      },
      "outputs": [],
      "source": [
        "# Build the network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \"\"\"Convolutional filter output size = ((w - f + 2p)/s) + 1\n",
        "        where w is width/height\n",
        "        f is filter/kernel size\n",
        "        p is padding \n",
        "        s is stride \"\"\"\n",
        "        \"\"\"Input shape = (256, 3, 150, 150)\"\"\"\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1)\n",
        "        \"\"\"Current shape = (256, 10, 150, 150)\"\"\"\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \"\"\"Current shape = (256, 10, 150, 150)\"\"\"\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=10)\n",
        "        \"\"\"Current shape = (256, 10, 150, 150)\"\"\"\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
        "        \"\"\"Current shape = (256, 20, 150, 150)\"\"\"\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \"\"\"Current shape = (256, 20, 150, 150)\"\"\"\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=20)\n",
        "        \"\"\"Current shape = (256, 20, 150, 150)\"\"\"\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        \"\"\"Current shape = (256, 20, 75, 75)\"\"\"\n",
        "        self.fc1 = nn.Linear(in_features=20*75*75, out_features=20*75)\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(in_features=20*75, out_features=num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv1(input.cuda())\n",
        "        output = self.relu1(output)\n",
        "        output = self.bn1(output)\n",
        "        output = self.conv2(output)\n",
        "        output = self.relu2(output)\n",
        "        output = self.bn2(output)\n",
        "        output = self.max_pool1(output)\n",
        "        output = output.view(-1, 20*75*75)\n",
        "        output = self.fc1(output)\n",
        "        output = self.dropout1(output)\n",
        "        output = self.fc2(output)\n",
        "        output = self.softmax(output)\n",
        "        return output\n",
        "    \n",
        "model = ConvNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzMgWHV44v6s",
        "outputId": "6a72a2e3-27d2-4f8b-87b6-d34e17354bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_count: 9469\n",
            "val_count: 3925\n"
          ]
        }
      ],
      "source": [
        "# Count the number of images in train and val\n",
        "train_count = 9469\n",
        "val_count = 3925\n",
        "print('train_count: ' + str(train_count))\n",
        "print('val_count: ' + str(val_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRJr2pLZ42tE",
        "outputId": "aa631491-5208-421b-b58c-4a3100a12165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "train loss: tensor(0.0293)\n",
            "train accuracy: 0.15556024923434364\n",
            "val accuracy: 0.16152866242038216\n",
            "\n",
            "epoch: 1\n",
            "train loss: tensor(0.0297)\n",
            "train accuracy: 0.1596789523708945\n",
            "val accuracy: 0.15566878980891719\n",
            "\n",
            "epoch: 2\n",
            "train loss: tensor(0.0302)\n",
            "train accuracy: 0.16960608300770938\n",
            "val accuracy: 0.17248407643312103\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Training stage\n",
        "best_accuracy = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_accuracy = 0.0\n",
        "    model.train()\n",
        "    \"\"\"Train batch by batch\"\"\"\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        \"\"\"zero gradients at the start of each batch\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "          images = Variable(images.cuda())\n",
        "          labels = Variable(labels.cuda())\n",
        "        optimizer.zero_grad()\n",
        "        \"\"\"Feed images into model and get output\"\"\"\n",
        "        outputs = model(images)\n",
        "        \"\"\"Calculate loss function\"\"\"\n",
        "        loss = loss_function(outputs, labels)\n",
        "        \"\"\"Back propagation\"\"\"\n",
        "        loss.backward()\n",
        "        \"\"\"optimize\"\"\"\n",
        "        optimizer.step()\n",
        "        train_loss = loss.cpu().data*images.size(0)\n",
        "        _,prediction = torch.max(outputs.data, 1)\n",
        "        train_accuracy += int(torch.sum(prediction==labels.data))\n",
        "    train_loss = train_loss/train_count\n",
        "    train_accuracy = train_accuracy/train_count\n",
        "\n",
        "    model.eval()\n",
        "    val_accuracy = 0.0\n",
        "    for i, (images, labels) in enumerate(val_loader):\n",
        "        if torch.cuda.is_available():\n",
        "          images = Variable(images.cuda())\n",
        "          labels = Variable(labels.cuda())\n",
        "        outputs = model(images)\n",
        "        _,prediction = torch.max(outputs.data, 1)\n",
        "        val_accuracy += int(torch.sum(prediction==labels.data))\n",
        "    val_accuracy = val_accuracy/val_count\n",
        "    \n",
        "    print('epoch: ' + str(epoch))\n",
        "    print('train loss: ' + str(train_loss))\n",
        "    print('train accuracy: ' + str(train_accuracy))\n",
        "    print('val accuracy: ' + str(val_accuracy))\n",
        "    print()\n",
        "\n",
        "    if val_accuracy > best_accuracy:\n",
        "        torch.save(model.state_dict(), 'checkpoint.model')\n",
        "        best_accuracy = val_accuracy"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e337efa0dbf50a479327a09a7ca80daa2467993c275d2b4497525709bdc18a22"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
