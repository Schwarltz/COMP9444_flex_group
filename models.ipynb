{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix2 Original using ReLU6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "    # basically resnet block\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False), # convolution on itself; in_channels->middle_channels\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False), # downsamples if stride=2 (e.g. 56^2 -> 28^2)\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False), # convolution on itself again; middle_channels-> out_channels\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU6(),\n",
    "    )\n",
    "\n",
    "    # MobileNetV2 block but normal convolution first\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=3, stride=1, padding=1, bias=False), # std convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False), # depth-wise convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.ReLU6(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False), # point wise convolution\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU6(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    # structure based off MobileNetV2 table\n",
    "    # [bs, 3, 224, 224]\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.ReLU6()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    # [bs, 32, 112,112]\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    # [bs, 16, 112,112]\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    # [bs, 24, 56, 56]\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    # [bs, 32, 28, 28]\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    # [bs, 64, 14, 14]\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    # [bs, 96, 14, 14]\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    # [bs, 160, 7, 7]\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    # [bs, 320, 7, 7]\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    # [bs, 1280, 7, 7]\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    # 1280 nodes\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix2 Mish with Dropout\n",
    "Changed ReLU6 to Mish since it appears to converge much faster in the early stages\n",
    "Dropout added within blocks with probabilty of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "    d_prob = 0.1\n",
    "\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False), # self conv\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False), # downsamples if stride=2\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, in_channels*expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(in_channels*expansion),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(in_channels*expansion, in_channels*expansion, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False),\n",
    "        nn.BatchNorm2d(in_channels*expansion),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(in_channels*expansion, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Dropout(d_prob),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix2 Mish Model Partial Dropout\n",
    "Removed dropout inside of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "   \n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False), # convolution on itself; in_channels->middle_channels\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False), # downsamples if stride=2 (e.g. 56^2 -> 28^2)\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False), # convolution on itself again; middle_channels-> out_channels\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    \n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=3, stride=1, padding=1, bias=False), # std convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False), # depth-wise convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False), # point wise convolution\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    # structure based off MobileNetV2 table\n",
    "    # [bs, 3, 224, 224]\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    # [bs, 32, 112,112]\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1) \n",
    "    # [bs, 16, 112,112]\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    # [bs, 24, 56, 56]\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    # [bs, 32, 28, 28]\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    # [bs, 64, 14, 14]\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    # [bs, 96, 14, 14]\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    # [bs, 160, 7, 7]\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    # [bs, 320, 7, 7]\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    # [bs, 1280, 7, 7]\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    # 1280 nodes\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix2 with proper mobilenetv2 block implemented\n",
    "\n",
    "Changed the first convolution in pass2 to a point-wise convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=1, stride=1, padding=0, bias=False), # changed this from a std 3x3 convolution to a 1x1 0 padding convolution\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Residual Mix2 Network\n",
    "\n",
    "Final Iteration of Model\n",
    "Modified the _make_layer function and made the residual pass forward more often.\n",
    "Appears to converge faster than previous iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride=1, expansion=6, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    middle_channels = in_channels*expansion\n",
    "\n",
    "    self.pass1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, 3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.pass2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, middle_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, middle_channels, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False),\n",
    "        nn.BatchNorm2d(middle_channels),\n",
    "        nn.Mish(),\n",
    "        nn.Conv2d(middle_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Mish(),\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x1 = self.pass1(x)\n",
    "    x2 = self.pass2(x)\n",
    "    x = x1 + x2\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "    x += identity # always adds identity\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.relu6 = nn.Mish()\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    \n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1 or in_channels != out_channels: # identity convolution now occurs at least once in every function call\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Example Implementation Used For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
    "    ):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(\n",
    "              in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n",
    "          ) # [_, int_channels, x,y]\n",
    "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        ) # [_, int_channels, x,y]\n",
    "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        ) # [_, int_channels*4, x, y]\n",
    "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(\n",
    "            block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2 built from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, expansion, downsample=None):\n",
    "    super(block, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(in_channels, in_channels*expansion, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(in_channels*expansion)\n",
    "    self.conv2 = nn.Conv2d(in_channels*expansion, in_channels*expansion, kernel_size=3, stride=stride, padding=1,groups=in_channels, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(in_channels*expansion)\n",
    "    self.conv3 = nn.Conv2d(in_channels*expansion, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "    self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    self.dropout = nn.Dropout2d(0.1)\n",
    "    self.relu6 = nn.ReLU6()\n",
    "    self.downsample = downsample\n",
    "  \n",
    "  def forward(self, x):\n",
    "    identity = x.clone()\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.bn3(x)\n",
    "    x = self.relu6(x)\n",
    "    if self.downsample is not None:\n",
    "      identity = self.downsample(identity)\n",
    "      x += identity\n",
    "    return x\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "  def __init__(self, in_channels=3, num_classes=10):\n",
    "    super(Network, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=7, stride=2, padding=3, bias=False) # initial convolution 112*112*64\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "    # self.block1 = block(32, 16, 1, expansion=1)\n",
    "    self.block1 = self._make_layer(block, 1, 32, 16, 1, 1)\n",
    "    self.block2 = self._make_layer(block, 6, 16, 24, 2, 2)\n",
    "    self.block3 = self._make_layer(block, 6, 24, 32, 3, 2)\n",
    "    self.block4 = self._make_layer(block, 6, 32, 64, 4, 2)\n",
    "    self.block5 = self._make_layer(block, 6, 64, 96, 3, 1)\n",
    "    self.block6 = self._make_layer(block, 6, 96, 160, 3, 2)\n",
    "    self.block7 = self._make_layer(block, 6, 160, 320, 1, 1)\n",
    "    self.conv2 = nn.Conv2d(320,1280, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn2 = nn.BatchNorm2d(1280)\n",
    "    self.avgPool = nn.AvgPool2d(7)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = nn.Linear(1280, num_classes)\n",
    "\n",
    "    self.dropout = nn.Dropout2d(0.1)\n",
    "    self.relu6 = nn.ReLU6()\n",
    "\n",
    "    pass\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.block1(x)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "    x = self.block6(x)\n",
    "    x = self.block7(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.relu6(x)\n",
    "    x = self.avgPool(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = F.log_softmax(x, dim=1)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "  def _make_layer(self, block, expansion, in_channels, out_channels, repeats, stride):\n",
    "    layers = []\n",
    "    downsample = None\n",
    "\n",
    "    if stride != 1:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
    "          nn.BatchNorm2d(out_channels),\n",
    "      )\n",
    "\n",
    "\n",
    "    layers.append(\n",
    "        block(in_channels, out_channels, stride, expansion, downsample)\n",
    "    )\n",
    "\n",
    "    for _ in range(repeats-1):\n",
    "      layers.append(\n",
    "          block(out_channels, out_channels, 1, expansion)\n",
    "      )\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d53ec6266a26bc8bdfb2a6f99d7ada5f09e50f34a6a9bd31567b89d722776c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
